import 'dart:io';
import 'dart:typed_data';
import 'dart:math' as math;
import 'package:tflite_flutter/tflite_flutter.dart';
import 'package:image/image.dart' as img;
import 'package:google_mlkit_face_detection/google_mlkit_face_detection.dart';
import 'dart:ui';

class FaceRecognitionService {
  static Interpreter? _interpreter;
  static bool _isInitialized = false;
  
  // Ø¯Ø¹Ù… embeddings Ù…ØªØ¹Ø¯Ø¯Ø© Ù„ÙƒÙ„ Ø´Ø®Øµ (3-10 ØµÙˆØ±)
  static Map<String, List<List<double>>> _storedMultipleEmbeddings = {};
  
  // Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Ù…ØªØºÙŠØ±Ø© Ø­Ø³Ø¨ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„)
  static const int INPUT_SIZE = 112;
  static int EMBEDDING_SIZE = 512; // ØªØºÙŠÙ‘Ø± ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø­Ø³Ø¨ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
  
  // Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ù…Ø­Ø³Ù‘Ù†Ø© Ù„Ù„Ø¯Ù‚Ø©
  static const double MIN_FACE_SIZE = 0.1;
  static const double DEFAULT_THRESHOLD = 0.25;
  
  /// ØªØ­Ø¯ÙŠØ« Ø­Ø¬Ù… Ø§Ù„Ù€ embedding ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
  static void _updateEmbeddingSize(int newSize) {
    EMBEDDING_SIZE = newSize;
    print('ğŸ“ Updated EMBEDDING_SIZE to: $newSize');
  }
  
  /// ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù…
  static Future<bool> initialize() async {
    if (_isInitialized) return true;
    
    try {
      print('ğŸš€ Loading face recognition model...');
      
      // Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© (float16 Ù„Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø£ÙØ¶Ù„)
      final modelPaths = [
        'assets/models/w600k_r50.tflite',
        'assets/models/1k3d68_float16.tflite',
        'assets/models/2d106det_float16.tflite',
        'assets/models/det_10g_simplified_float16.tflite',
        // Ù†Ø³Ø® float32 ÙƒÙ€ backup
        'assets/models/1k3d68_float32.tflite',
        'assets/models/2d106det_float32.tflite',
        'assets/models/det_10g_simplified_float32.tflite',
      ];
      
      // Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„ Ø£ÙŠ Ù…ÙˆØ¯ÙŠÙ„ Ù…ØªÙˆÙØ±
      for (String path in modelPaths) {
        try {
          _interpreter = await Interpreter.fromAsset(path);
          print('âœ… Successfully loaded model from: $path');
          
          final inputDetails = _interpreter!.getInputTensor(0);
          final outputDetails = _interpreter!.getOutputTensor(0);
          print('ğŸ“Š Input shape: ${inputDetails.shape}, type: ${inputDetails.type}');
          print('ğŸ“Š Output shape: ${outputDetails.shape}, type: ${outputDetails.type}');
          
          // ØªØ­Ø¯ÙŠØ« EMBEDDING_SIZE ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ù…Ù† Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
          if (outputDetails.shape.length == 4) {
            // Shape: [1, 1, 1, 512]
            _updateEmbeddingSize(outputDetails.shape[3]);
          } else if (outputDetails.shape.length == 2) {
            // Shape: [1, 512]
            _updateEmbeddingSize(outputDetails.shape[1]);
          } else if (outputDetails.shape.length == 1) {
            // Shape: [512]
            _updateEmbeddingSize(outputDetails.shape[0]);
          }
          
          _isInitialized = true;
          return true;
        } catch (e) {
          print('âš ï¸ Failed to load $path: $e');
          continue;
        }
      }
      
      print('âŒ All models failed to load');
      print('ğŸ’¡ Make sure you added the models in pubspec.yaml under assets:');
      print('   flutter:');
      print('     assets:');
      print('       - assets/models/');
      return false;
    } catch (e) {
      print('âŒ Initialization error: $e');
      return false;
    }
  }

  /// ØªØ­Ø³ÙŠÙ† Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø§Ø­ØªØ±Ø§ÙÙŠØ©
  static img.Image _enhanceImage(img.Image image) {
    // 1. ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ¨Ø§ÙŠÙ† (Contrast enhancement)
    image = img.adjustColor(image, contrast: 1.2);
    
    // 2. ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¥Ø¶Ø§Ø¡Ø© (Brightness adjustment)
    image = img.adjustColor(image, brightness: 1.05);
    
    // 3. ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ù„ÙˆØ§Ù† (Color correction)
    image = img.adjustColor(
      image,
      saturation: 1.1,
      brightness: 1.02,
      contrast: 1.1,
    );
    
    // 4. Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ÙˆØ¶ÙˆØ­ (Sharpening) - Ø·Ø±ÙŠÙ‚Ø© Ù…Ø­Ø³Ù‘Ù†Ø©
    try {
      // Ø§Ø³ØªØ®Ø¯Ø§Ù… gaussian blur Ø«Ù… Ø·Ø±Ø­Ù‡ Ù…Ù† Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ© (Unsharp Mask)
      final blurred = img.gaussianBlur(image, radius: 1);
      
      // ØªØ·Ø¨ÙŠÙ‚ unsharp mask ÙŠØ¯ÙˆÙŠØ§Ù‹
      for (int y = 0; y < image.height; y++) {
        for (int x = 0; x < image.width; x++) {
          final original = image.getPixel(x, y);
          final blurredPixel = blurred.getPixel(x, y);
          
          // ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø©: sharpened = original + amount * (original - blurred)
          final amount = 1.5;
          final r = (original.r + amount * (original.r - blurredPixel.r)).clamp(0, 255).toInt();
          final g = (original.g + amount * (original.g - blurredPixel.g)).clamp(0, 255).toInt();
          final b = (original.b + amount * (original.b - blurredPixel.b)).clamp(0, 255).toInt();
          
          image.setPixel(x, y, img.ColorRgb8(r, g, b));
        }
      }
    } catch (e) {
      // Ø¥Ø°Ø§ ÙØ´Ù„ Ø§Ù„Ù€ sharpeningØŒ Ù†ÙƒÙ…Ù„ Ø¨Ø¯ÙˆÙ†Ù‡
      print('âš ï¸ Sharpening skipped: $e');
    }
    
    return image;
  }

  /// Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±Ø© Ù‚Ø¨Ù„ Ø¥Ø¯Ø®Ø§Ù„Ù‡Ø§ Ù„Ù„Ù…ÙˆØ¯ÙŠÙ„
  static Float32List preprocessImage(
    img.Image faceImage,
    {String normalizationType = 'arcface'}
  ) {
    // ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„ØµÙˆØ±Ø© Ø£ÙˆÙ„Ø§Ù‹
    var processedImage = _enhanceImage(faceImage);
    
    // ØªØºÙŠÙŠØ± Ø§Ù„Ø­Ø¬Ù… Ø¥Ù„Ù‰ 112x112
    processedImage = img.copyResize(
      processedImage,
      width: INPUT_SIZE,
      height: INPUT_SIZE,
      interpolation: img.Interpolation.cubic,
    );
    
    final input = Float32List(INPUT_SIZE * INPUT_SIZE * 3);
    int pixelIndex = 0;
    
    // ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ array Ù…Ø¹ normalization
    for (int y = 0; y < INPUT_SIZE; y++) {
      for (int x = 0; x < INPUT_SIZE; x++) {
        final pixel = processedImage.getPixel(x, y);
        
        // Ø£Ù†ÙˆØ§Ø¹ Ù…Ø®ØªÙ„ÙØ© Ù…Ù† Normalization Ø­Ø³Ø¨ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
        switch (normalizationType.toLowerCase()) {
          case 'arcface':
            input[pixelIndex] = (pixel.r / 127.5) - 1.0;
            input[pixelIndex + 1] = (pixel.g / 127.5) - 1.0;
            input[pixelIndex + 2] = (pixel.b / 127.5) - 1.0;
            break;
          case 'facenet':
            input[pixelIndex] = (pixel.r - 127.5) / 128.0;
            input[pixelIndex + 1] = (pixel.g - 127.5) / 128.0;
            input[pixelIndex + 2] = (pixel.b - 127.5) / 128.0;
            break;
          case 'imagenet':
            input[pixelIndex] = pixel.r / 255.0;
            input[pixelIndex + 1] = pixel.g / 255.0;
            input[pixelIndex + 2] = pixel.b / 255.0;
            break;
          default:
            input[pixelIndex] = (pixel.r / 127.5) - 1.0;
            input[pixelIndex + 1] = (pixel.g / 127.5) - 1.0;
            input[pixelIndex + 2] = (pixel.b / 127.5) - 1.0;
        }
        pixelIndex += 3;
      }
    }
    
    return input;
  }

  /// ØªÙˆÙ„ÙŠØ¯ embedding Ù„ØµÙˆØ±Ø© ÙˆØ¬Ù‡
  static Future<List<double>?> generateEmbedding(
    File imageFile,
    {String normalizationType = 'arcface'}
  ) async {
    if (!_isInitialized) {
      final initialized = await initialize();
      if (!initialized) return null;
    }
    
    try {
      if (!await imageFile.exists()) {
        print('âŒ Image file not found: ${imageFile.path}');
        return null;
      }
      
      print('ğŸ” Starting face detection...');
      final faceRect = await detectFaceEnhanced(imageFile);
      if (faceRect == null) {
        print('âŒ No face detected');
        return null;
      }
      
      print('âœ‚ï¸ Cropping face...');
      final croppedFace = await cropFaceEnhanced(imageFile, faceRect);
      if (croppedFace == null) {
        print('âŒ Face cropping failed');
        return null;
      }
      
      print('ğŸ¨ Preprocessing image...');
      final input = preprocessImage(croppedFace, normalizationType: normalizationType);
      final inputTensor = input.reshape([1, INPUT_SIZE, INPUT_SIZE, 3]);
      
      final outputShape = _interpreter!.getOutputTensor(0).shape;
      print('ğŸ“Š Model output shape: $outputShape');

      List<double> rawEmbedding;
      final stopwatch = Stopwatch()..start();

      // Ù…Ø¹Ø§Ù„Ø¬Ø© output Ø­Ø³Ø¨ Ø´ÙƒÙ„Ù‡
      if (outputShape.length == 4) {
        // Shape: [1, 1, 1, EMBEDDING_SIZE]
        print('ğŸ”§ Using 4D output structure');
        final embeddingSize = outputShape[3];
        
        final outputTensor = List.generate(
          outputShape[0],
          (i) => List.generate(
            outputShape[1],
            (j) => List.generate(
              outputShape[2],
              (k) => List.filled(embeddingSize, 0.0),
            ),
          ),
        );
        
        print('ğŸš€ Running model inference...');
        _interpreter!.run(inputTensor, outputTensor);
        stopwatch.stop();
        
        print('â±ï¸ Inference time: ${stopwatch.elapsedMilliseconds}ms');
        rawEmbedding = List<double>.from(outputTensor[0][0][0]);
        print('âœ… Extracted ${rawEmbedding.length} values from 4D output');
        
      } else if (outputShape.length == 2) {
        // Shape: [1, EMBEDDING_SIZE]
        print('ğŸ”§ Using 2D output structure');
        final embeddingSize = outputShape[1];
        final outputTensor = List.generate(1, (i) => List.filled(embeddingSize, 0.0));
        
        print('ğŸš€ Running model inference...');
        _interpreter!.run(inputTensor, outputTensor);
        stopwatch.stop();
        
        print('â±ï¸ Inference time: ${stopwatch.elapsedMilliseconds}ms');
        rawEmbedding = List<double>.from(outputTensor[0]);
        print('âœ… Extracted ${rawEmbedding.length} values from 2D output');
        
      } else if (outputShape.length == 1) {
        // Shape: [EMBEDDING_SIZE]
        print('ğŸ”§ Using 1D output structure');
        final embeddingSize = outputShape[0];
        final outputTensor = List.filled(embeddingSize, 0.0);
        
        print('ğŸš€ Running model inference...');
        _interpreter!.run(inputTensor, outputTensor);
        stopwatch.stop();
        
        print('â±ï¸ Inference time: ${stopwatch.elapsedMilliseconds}ms');
        rawEmbedding = List<double>.from(outputTensor);
        print('âœ… Extracted ${rawEmbedding.length} values from 1D output');
        
      } else {
        print('âŒ Unsupported output shape: $outputShape');
        return null;
      }
      
      print('ğŸ“ Normalizing embedding...');
      final normalizedEmbedding = _normalizeEmbeddingEnhanced(rawEmbedding);
      
      print('âœ… Embedding generated successfully: ${normalizedEmbedding.length} dimensions');
      return normalizedEmbedding;
      
    } catch (e, stackTrace) {
      print('âŒ Embedding generation error: $e');
      print('Stack trace: $stackTrace');
      return null;
    }
  }

  /// ÙƒØ´Ù Ø§Ù„ÙˆØ¬Ù‡ ÙÙŠ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Google ML Kit
  static Future<Rect?> detectFaceEnhanced(File imageFile) async {
    final options = FaceDetectorOptions(
      enableContours: false,
      enableClassification: false,
      enableLandmarks: true,
      enableTracking: false,
      minFaceSize: MIN_FACE_SIZE,
      performanceMode: FaceDetectorMode.accurate,
    );
    
    final faceDetector = FaceDetector(options: options);
    final inputImage = InputImage.fromFile(imageFile);
    final faces = await faceDetector.processImage(inputImage);
    faceDetector.close();
    
    if (faces.isNotEmpty) {
      print('ğŸ‘¤ Detected ${faces.length} faces');
      
      Face? bestFace;
      double bestScore = 0;
      
      // Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ ÙˆØ¬Ù‡ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø¬ÙˆØ¯Ø©
      for (Face face in faces) {
        double qualityScore = _calculateFaceQuality(face);
        if (qualityScore > bestScore) {
          bestScore = qualityScore;
          bestFace = face;
        }
      }
      
      if (bestFace != null) {
        print('âœ… Selected best face with score: ${bestScore.toStringAsFixed(2)}');
        return bestFace.boundingBox;
      }
    }
    
    return null;
  }

  /// Ø­Ø³Ø§Ø¨ Ø¬ÙˆØ¯Ø© Ø§Ù„ÙˆØ¬Ù‡ Ø§Ù„Ù…ÙƒØªØ´Ù
  static double _calculateFaceQuality(Face face) {
    double score = 0;
    
    // Ø­Ø¬Ù… Ø§Ù„ÙˆØ¬Ù‡ (ÙƒÙ„Ù…Ø§ Ø£ÙƒØ¨Ø± ÙƒÙ„Ù…Ø§ Ø£ÙØ¶Ù„)
    final faceArea = face.boundingBox.width * face.boundingBox.height;
    score += math.min(faceArea / 10000, 1.0) * 30;
    
    // Ø²Ø§ÙˆÙŠØ© Ø§Ù„Ø±Ø£Ø³ Ø§Ù„Ø£ÙÙ‚ÙŠØ© (ÙƒÙ„Ù…Ø§ Ø£Ù‚Ù„ ÙƒÙ„Ù…Ø§ Ø£ÙØ¶Ù„)
    if (face.headEulerAngleY != null) {
      score += (90 - face.headEulerAngleY!.abs()) / 90 * 25;
    }
    
    // Ø²Ø§ÙˆÙŠØ© Ø§Ù„Ø±Ø£Ø³ Ø§Ù„Ø¹Ù…ÙˆØ¯ÙŠØ©
    if (face.headEulerAngleZ != null) {
      score += (90 - face.headEulerAngleZ!.abs()) / 90 * 25;
    }
    
    // Ø¹Ø¯Ø¯ Ù†Ù‚Ø§Ø· Ø§Ù„ÙˆØ¬Ù‡ Ø§Ù„Ù…ÙƒØªØ´ÙØ©
    if (face.landmarks.isNotEmpty) {
      score += math.min(face.landmarks.length / 10, 1.0) * 20;
    }
    
    return score;
  }

  /// Ù‚Øµ Ø§Ù„ÙˆØ¬Ù‡ Ù…Ù† Ø§Ù„ØµÙˆØ±Ø© Ù…Ø¹ padding Ù…Ø­Ø³Ù‘Ù†
  static Future<img.Image?> cropFaceEnhanced(File imageFile, Rect faceRect) async {
    try {
      final imageBytes = await imageFile.readAsBytes();
      final originalImage = img.decodeImage(imageBytes);
      if (originalImage == null) return null;
      
      final faceSize = math.max(faceRect.width, faceRect.height);
      final paddingRatio = _calculateOptimalPadding(faceSize);
      final padding = (faceSize * paddingRatio).toInt();
      
      final x = math.max(0, faceRect.left.toInt() - padding);
      final y = math.max(0, faceRect.top.toInt() - padding);
      final maxWidth = originalImage.width - x;
      final maxHeight = originalImage.height - y;
      final width = math.min(maxWidth, faceRect.width.toInt() + (padding * 2));
      final height = math.min(maxHeight, faceRect.height.toInt() + (padding * 2));
      
      if (width <= 0 || height <= 0) return null;
      
      var croppedImage = img.copyCrop(
        originalImage,
        x: x,
        y: y,
        width: width,
        height: height,
      );
      
      // Ø¬Ø¹Ù„ Ø§Ù„ØµÙˆØ±Ø© Ù…Ø±Ø¨Ø¹Ø© (square)
      final targetSize = math.max(croppedImage.width, croppedImage.height);
      final squareImage = img.Image(width: targetSize, height: targetSize);
      img.fill(squareImage, color: img.ColorRgb8(128, 128, 128));
      
      final offsetX = (targetSize - croppedImage.width) ~/ 2;
      final offsetY = (targetSize - croppedImage.height) ~/ 2;
      img.compositeImage(squareImage, croppedImage, dstX: offsetX, dstY: offsetY);
      
      return squareImage;
      
    } catch (e) {
      print('âŒ Enhanced face cropping error: $e');
      return null;
    }
  }

  /// Ø­Ø³Ø§Ø¨ padding Ù…Ø«Ø§Ù„ÙŠ Ø­Ø³Ø¨ Ø­Ø¬Ù… Ø§Ù„ÙˆØ¬Ù‡
  static double _calculateOptimalPadding(double faceSize) {
    if (faceSize < 100) return 0.5;
    if (faceSize < 200) return 0.35;
    if (faceSize < 400) return 0.25;
    return 0.15;
  }

  /// ØªØ·Ø¨ÙŠØ¹ embedding (L2 Normalization)
  static List<double> _normalizeEmbeddingEnhanced(List<double> embedding) {
    double norm = 0.0;
    for (double value in embedding) {
      norm += value * value;
    }
    norm = math.sqrt(norm);
    
    if (norm == 0.0 || norm.isNaN || norm.isInfinite) {
      print('âš ï¸ Warning: Invalid norm value: $norm');
      return embedding;
    }
    
    final normalized = embedding.map((value) => value / norm).toList();
    
    // Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù†ØªÙŠØ¬Ø©
    double checkNorm = 0.0;
    for (double value in normalized) {
      if (value.isNaN || value.isInfinite) {
        print('âš ï¸ Warning: Invalid normalized value detected');
        return embedding;
      }
      checkNorm += value * value;
    }
    
    return normalized;
  }

  /// Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† embedding vectors (Cosine Similarity)
  static double calculateSimilarity(List<double> embedding1, List<double> embedding2) {
    if (embedding1.length != embedding2.length) {
      throw ArgumentError('Embedding length mismatch: ${embedding1.length} vs ${embedding2.length}');
    }
    
    double dotProduct = 0.0;
    double norm1 = 0.0;
    double norm2 = 0.0;
    
    for (int i = 0; i < embedding1.length; i++) {
      dotProduct += embedding1[i] * embedding2[i];
      norm1 += embedding1[i] * embedding1[i];
      norm2 += embedding2[i] * embedding2[i];
    }
    
    norm1 = math.sqrt(norm1);
    norm2 = math.sqrt(norm2);
    
    if (norm1 == 0.0 || norm2 == 0.0) return 0.0;
    
    final similarity = dotProduct / (norm1 * norm2);
    return math.max(0.0, math.min(1.0, similarity));
  }

  /// Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ ÙˆØ¬Ù‡ Ù…Ø¹ Ø¯Ø¹Ù… embeddings Ù…ØªØ¹Ø¯Ø¯Ø©
  static Future<RecognitionResult?> recognizeFace(
    File imageFile, {
    double threshold = DEFAULT_THRESHOLD,
    String normalizationType = 'arcface',
    bool useAdaptiveThreshold = true,
  }) async {
    print('=== ğŸ” Enhanced Face Recognition (Multi-Embedding) ===');
    print('ğŸ“Š Using threshold: ${(threshold * 100).toStringAsFixed(1)}%, normalization: $normalizationType');
    
    final queryEmbedding = await generateEmbedding(imageFile, normalizationType: normalizationType);
    if (queryEmbedding == null) {
      print('âŒ Failed to generate query embedding');
      return null;
    }
    
    if (_storedMultipleEmbeddings.isEmpty) {
      print('âš ï¸ No stored embeddings available');
      return RecognitionResult(personId: 'unknown', similarity: 0.0, isMatch: false);
    }
    
    String? bestMatchId;
    double highestSimilarity = -1.0;
    Map<String, double> personBestSimilarities = {};
    
    print('ğŸ” Comparing with ${_storedMultipleEmbeddings.length} persons...');
    
    // Ù…Ù‚Ø§Ø±Ù†Ø© Ù…Ø¹ ÙƒÙ„ embeddings Ù„ÙƒÙ„ Ø´Ø®Øµ
    for (var personEntry in _storedMultipleEmbeddings.entries) {
      String personId = personEntry.key;
      List<List<double>> personEmbeddings = personEntry.value;
      
      double personBestSimilarity = -1.0;
      
      // Ø¥ÙŠØ¬Ø§Ø¯ Ø£ÙØ¶Ù„ ØªØ·Ø§Ø¨Ù‚ Ù…Ù† Ø¨ÙŠÙ† ÙƒÙ„ embeddings Ø§Ù„Ø´Ø®Øµ
      for (int i = 0; i < personEmbeddings.length; i++) {
        try {
          final similarity = calculateSimilarity(queryEmbedding, personEmbeddings[i]);
          
          if (similarity > personBestSimilarity) {
            personBestSimilarity = similarity;
          }
        } catch (e) {
          print('âš ï¸ Error comparing with $personId embedding $i: $e');
        }
      }
      
      personBestSimilarities[personId] = personBestSimilarity;
      print('  ğŸ‘¤ $personId: ${(personBestSimilarity * 100).toStringAsFixed(1)}% (from ${personEmbeddings.length} embeddings)');
      
      if (personBestSimilarity > highestSimilarity) {
        highestSimilarity = personBestSimilarity;
        bestMatchId = personId;
      }
    }
    
    // Adaptive threshold (Ø°ÙƒÙŠ)
    double finalThreshold = threshold;
    if (useAdaptiveThreshold && personBestSimilarities.isNotEmpty) {
      var sortedSimilarities = personBestSimilarities.values.toList()..sort((a, b) => b.compareTo(a));
      
      if (sortedSimilarities.length > 1) {
        final secondHighest = sortedSimilarities[1];
        final gap = highestSimilarity - secondHighest;
        
        // Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ ÙØ¬ÙˆØ© ÙƒØ¨ÙŠØ±Ø© Ø¨ÙŠÙ† Ø§Ù„Ø£ÙˆÙ„ ÙˆØ§Ù„Ø«Ø§Ù†ÙŠ
        if (gap > 0.15) {
          finalThreshold = math.min(threshold, highestSimilarity - 0.05);
          print('ğŸ¯ Adaptive threshold applied: ${(finalThreshold * 100).toStringAsFixed(1)}% (gap: ${(gap * 100).toStringAsFixed(1)}%)');
        }
      }
    }
    
    final isMatch = highestSimilarity >= finalThreshold;
    
    if (isMatch) {
      print('âœ… MATCH FOUND: $bestMatchId (${(highestSimilarity * 100).toStringAsFixed(1)}%)');
    } else {
      print('âŒ NO MATCH: Best was ${(highestSimilarity * 100).toStringAsFixed(1)}% < ${(finalThreshold * 100).toStringAsFixed(1)}%');
    }
    
    return RecognitionResult(
      personId: bestMatchId ?? 'unknown',
      similarity: highestSimilarity,
      isMatch: isMatch,
      threshold: finalThreshold,
    );
  }

  /// ØªØ®Ø²ÙŠÙ† embedding Ø¬Ø¯ÙŠØ¯ Ù„Ø´Ø®Øµ
  static Future<bool> storeFaceEmbedding(
    String personId,
    File imageFile,
    {String normalizationType = 'arcface'}
  ) async {
    final embedding = await generateEmbedding(imageFile, normalizationType: normalizationType);
    if (embedding != null && embedding.isNotEmpty) {
      // Ø¥Ø°Ø§ Ø§Ù„Ø´Ø®Øµ Ù…ÙˆØ¬ÙˆØ¯ØŒ Ù†Ø¶ÙŠÙ embedding Ø¬Ø¯ÙŠØ¯
      if (_storedMultipleEmbeddings.containsKey(personId)) {
        _storedMultipleEmbeddings[personId]!.add(embedding);
        print('âœ… Added embedding #${_storedMultipleEmbeddings[personId]!.length} for $personId');
      } else {
        // Ø¥Ø°Ø§ Ø§Ù„Ø´Ø®Øµ Ø¬Ø¯ÙŠØ¯ØŒ Ù†Ù†Ø´Ø¦ Ù‚Ø§Ø¦Ù…Ø© Ø¬Ø¯ÙŠØ¯Ø©
        _storedMultipleEmbeddings[personId] = [embedding];
        print('âœ… Created new person $personId with first embedding');
      }
      
      print('ğŸ“Š $personId now has ${_storedMultipleEmbeddings[personId]!.length} embeddings');
      return true;
    }
    print('âŒ Failed to store embedding for $personId');
    return false;
  }

  /// ØªØ­Ù…ÙŠÙ„ embeddings Ù…ØªØ¹Ø¯Ø¯Ø© Ù…Ù† Firestore
  static void loadMultipleEmbeddings(Map<String, List<List<double>>> embeddings) {
    _storedMultipleEmbeddings = Map.from(embeddings);
    int totalEmbeddings = 0;
    embeddings.forEach((personId, embList) {
      totalEmbeddings += embList.length;
      print('  ğŸ‘¤ $personId: ${embList.length} embeddings');
    });
    print('âœ… Loaded ${embeddings.length} persons with $totalEmbeddings total embeddings');
  }

  /// Ø¥Ø±Ø¬Ø§Ø¹ embeddings Ø¨ØµÙŠØºØ© Firestore
  static Map<String, dynamic> getStoredEmbeddings() {
    Map<String, dynamic> result = {};
    _storedMultipleEmbeddings.forEach((personId, embeddings) {
      result[personId] = embeddings;
    });
    print('ğŸ“¤ Exporting ${result.length} persons');
    return result;
  }

  /// Ø­Ø°Ù Ø¬Ù…ÙŠØ¹ embeddings Ù„Ø´Ø®Øµ
  static void removeFaceEmbedding(String personId) {
    final removed = _storedMultipleEmbeddings.remove(personId);
    if (removed != null) {
      print('ğŸ—‘ï¸ Removed $personId (${removed.length} embeddings)');
    } else {
      print('âš ï¸ Person $personId not found');
    }
  }

  /// Ù…Ø³Ø­ ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
  static void clearStoredEmbeddings() {
    final count = _storedMultipleEmbeddings.length;
    _storedMultipleEmbeddings.clear();
    print('ğŸ—‘ï¸ Cleared all $count persons');
  }

  /// ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯
  static void dispose() {
    _interpreter?.close();
    _interpreter = null;
    _isInitialized = false;
    _storedMultipleEmbeddings.clear();
    print('ğŸ”Œ Face recognition service disposed');
  }

  /// Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù…ÙÙŠØ¯Ø©
  static Map<String, dynamic> getStatistics() {
    int totalEmbeddings = 0;
    Map<String, int> embeddingCounts = {};
    
    _storedMultipleEmbeddings.forEach((personId, embeddings) {
      embeddingCounts[personId] = embeddings.length;
      totalEmbeddings += embeddings.length;
    });
    
    return {
      'total_persons': _storedMultipleEmbeddings.length,
      'total_embeddings': totalEmbeddings,
      'average_embeddings_per_person': _storedMultipleEmbeddings.isEmpty
          ? 0
          : (totalEmbeddings / _storedMultipleEmbeddings.length).toStringAsFixed(1),
      'embedding_counts': embeddingCounts,
      'current_embedding_size': EMBEDDING_SIZE,
    };
  }
}

/// Ù†ØªÙŠØ¬Ø© Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ø±Ù
class RecognitionResult {
  final String personId;
  final double similarity;
  final bool isMatch;
  final double threshold;
  
  RecognitionResult({
    required this.personId,
    required this.similarity,
    required this.isMatch,
    this.threshold = 0.25,
  });
  
  @override
  String toString() {
    return 'RecognitionResult(personId: $personId, similarity: ${(similarity * 100).toStringAsFixed(1)}%, isMatch: $isMatch, threshold: ${(threshold * 100).toStringAsFixed(1)}%)';
  }
}