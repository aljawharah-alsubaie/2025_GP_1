import 'dart:io';
import 'dart:typed_data';
import 'dart:math' as math;
import 'package:tflite_flutter/tflite_flutter.dart';
import 'package:image/image.dart' as img;
import 'dart:ui';

class InsightFacePipeline {
  // Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„Ø«Ù„Ø§Ø«Ø©
  static Interpreter? _detectionModel; // det_10g
  static Interpreter? _landmarkModel; // 1k3d68
  static Interpreter? _recognitionModel; // w600k_r50
  static bool _isInitialized = false;

  // Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
  static const int DETECTION_INPUT_SIZE = 640;
  static const int LANDMARK_INPUT_SIZE = 192;
  static const int RECOGNITION_INPUT_SIZE = 112;
  static int EMBEDDING_SIZE = 512;

  static Map<String, List<List<double>>> _storedMultipleEmbeddings = {};
  static const double DEFAULT_THRESHOLD = 0.35;

  /// ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„Ø«Ù„Ø§Ø«Ø©
  static Future<bool> initialize() async {
    if (_isInitialized) return true;

    try {
      print('ğŸš€ Loading InsightFace Pipeline (3 models)...');

      // 1ï¸âƒ£ ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ¯ÙŠÙ„ Face Detection
      print('ğŸ“¦ Loading detection model...');
      try {
        _detectionModel = await Interpreter.fromAsset(
          'assets/models/det_10g_simplified_float16.tflite',
          options: InterpreterOptions()..threads = 4,
        );

        // Ø·Ø¨Ø§Ø¹Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
        final inputShape = _detectionModel!.getInputTensor(0).shape;
        final outputShape = _detectionModel!.getOutputTensor(0).shape;
        print('âœ… Detection model loaded');
        print('   Input shape: $inputShape');
        print('   Output shape: $outputShape');
      } catch (e) {
        print('âŒ Detection model failed: $e');
        print(
          'âš ï¸ Make sure the model file exists at: assets/models/det_10g_simplified_float16.tflite',
        );
        return false;
      }

      // 2ï¸âƒ£ ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ¯ÙŠÙ„ Landmarks
      print('ğŸ“¦ Loading landmark model...');
      try {
        _landmarkModel = await Interpreter.fromAsset(
          'assets/models/2d106det_float16.tflite',
          options: InterpreterOptions()..threads = 4,
        );

        final inputShape = _landmarkModel!.getInputTensor(0).shape;
        final outputShape = _landmarkModel!.getOutputTensor(0).shape;
        print('âœ… Landmark model loaded');
        print('   Input shape: $inputShape');
        print('   Output shape: $outputShape');
      } catch (e) {
        print('âŒ Landmark model failed: $e');
        print(
          'âš ï¸ Make sure the model file exists at: assets/models/2d106det_float16.tflite',
        );
        return false;
      }

      // 3ï¸âƒ£ ØªØ­Ù…ÙŠÙ„ Ù…ÙˆØ¯ÙŠÙ„ Recognition
      print('ğŸ“¦ Loading recognition model...');
      try {
        _recognitionModel = await Interpreter.fromAsset(
          'assets/models/w600k_r50_float16.tflite',
          options: InterpreterOptions()..threads = 4,
        );

        final inputShape = _recognitionModel!.getInputTensor(0).shape;
        final outputShape = _recognitionModel!.getOutputTensor(0).shape;
        print('âœ… Recognition model loaded');
        print('   Input shape: $inputShape');
        print('   Output shape: $outputShape');

        if (outputShape.length == 2) {
          EMBEDDING_SIZE = outputShape[1];
        } else if (outputShape.length == 4) {
          EMBEDDING_SIZE = outputShape[3];
        }
        print('   Embedding size: $EMBEDDING_SIZE');
      } catch (e) {
        print('âŒ Recognition model failed: $e');
        print(
          'âš ï¸ Make sure the model file exists at: assets/models/w600k_r50_float16.tflite',
        );
        return false;
      }

      _isInitialized = true;
      print('âœ… InsightFace Pipeline initialized successfully!');
      print('=' * 50);
      return true;
    } catch (e) {
      print('âŒ Pipeline initialization error: $e');
      return false;
    }
  }

  /// ğŸ†• ÙƒØ´Ù ÙˆØ¬Ù‡ ÙˆØ§Ø­Ø¯ (Ù…ØªÙˆØ§ÙÙ‚ Ù…Ø¹ face_management)
  static Future<Rect?> detectFace(File imageFile) async {
    final faces = await detectFaces(imageFile);
    if (faces == null || faces.isEmpty) return null;
    return faces[0]; // Ø£ÙˆÙ„ ÙˆØ¬Ù‡ ÙÙ‚Ø·
  }

  /// 1ï¸âƒ£ Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: Face Detection
  static Future<List<Rect>?> detectFaces(
    File imageFile, {
    bool useFallback = true,
  }) async {
    try {
      print('ğŸ” Stage 1: Face Detection');

      if (_detectionModel == null) {
        print('âŒ Detection model not initialized');
        await initialize();
        if (_detectionModel == null) {
          if (useFallback) {
            print('âš ï¸ Model not available, using image fallback');
            return _createFallbackFace(imageFile);
          }
          return null;
        }
      }

      final imageBytes = await imageFile.readAsBytes();
      final originalImage = img.decodeImage(imageBytes);
      if (originalImage == null) {
        print('âŒ Failed to decode image');
        return null;
      }

      print(
        'ğŸ“ Original image size: ${originalImage.width}x${originalImage.height}',
      );

      // ØªØºÙŠÙŠØ± Ø§Ù„Ø­Ø¬Ù… Ù„Ù€ 640x640
      final resized = img.copyResize(
        originalImage,
        width: DETECTION_INPUT_SIZE,
        height: DETECTION_INPUT_SIZE,
        interpolation: img.Interpolation.cubic,
      );

      // ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ tensor
      final input = _imageToFloat32List(resized, DETECTION_INPUT_SIZE);
      final inputTensor = input.reshape([
        1,
        DETECTION_INPUT_SIZE,
        DETECTION_INPUT_SIZE,
        3,
      ]);

      // ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
      final outputShape = _detectionModel?.getOutputTensor(0).shape;
      if (outputShape == null) {
        print('âŒ Cannot get output tensor shape');
        return null;
      }
      print('ğŸ“Š Detection output shape: $outputShape');

      // ØªØ­Ø¯ÙŠØ¯ Ø´ÙƒÙ„ Ø§Ù„Ù€ output Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
      dynamic output;
      if (outputShape.length == 4) {
        // Ø´ÙƒÙ„ [1, height, width, channels]
        output = List.generate(
          outputShape[0],
          (i) => List.generate(
            outputShape[1],
            (j) => List.generate(
              outputShape[2],
              (k) => List.filled(outputShape[3], 0.0),
            ),
          ),
        );
      } else if (outputShape.length == 3) {
        // Ø´ÙƒÙ„ [1, num_detections, 15] Ø£Ùˆ Ù…Ø´Ø§Ø¨Ù‡
        output = List.generate(
          outputShape[0],
          (i) => List.generate(
            outputShape[1],
            (j) => List.filled(outputShape[2], 0.0),
          ),
        );
      } else if (outputShape.length == 2) {
        // Ø´ÙƒÙ„ [num_detections, 15]
        output = List.generate(
          outputShape[0],
          (i) => List.filled(outputShape[1], 0.0),
        );
      } else {
        print('âŒ Unsupported detection output shape: $outputShape');
        return null;
      }

      try {
        _detectionModel?.run(inputTensor, output);
      } catch (e) {
        print('âŒ Model run error: $e');
        if (useFallback) {
          print('ğŸ”„ Falling back to full image');
          return _createFallbackFace(imageFile);
        }
        return null;
      }
      print('âœ… Model inference completed');

      // Ø§Ø³ØªØ®Ø±Ø§Ø¬ bounding boxes
      List<Rect> faces = _parseFaceDetections(
        output,
        originalImage.width,
        originalImage.height,
        outputShape,
      );

      if (faces.isEmpty) {
        print('âš ï¸ No faces found after parsing');
        if (useFallback) {
          print('ğŸ”„ Using FALLBACK: treating whole image as face');

          // Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙƒØ§Ù…Ù„ Ø§Ù„ØµÙˆØ±Ø© Ù…Ø¹ padding ØµØºÙŠØ±
          final width = originalImage.width.toDouble();
          final height = originalImage.height.toDouble();

          // Ù†Ø³ØªØ®Ø¯Ù… 90% Ù…Ù† Ø§Ù„ØµÙˆØ±Ø© Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø­ÙˆØ§Ù
          final padding = 0.05;
          final paddedWidth = width * (1.0 - padding * 2);
          final paddedHeight = height * (1.0 - padding * 2);
          final paddedLeft = width * padding;
          final paddedTop = height * padding;

          faces.add(
            Rect.fromLTWH(paddedLeft, paddedTop, paddedWidth, paddedHeight),
          );

          print(
            'âœ… Fallback face region: ${paddedWidth.toInt()}x${paddedHeight.toInt()}',
          );
          print('   Position: (${paddedLeft.toInt()}, ${paddedTop.toInt()})');
        }
      }

      print('âœ… Detected ${faces.length} face(s)');
      return faces;
    } catch (e, stackTrace) {
      print('âŒ Face detection error: $e');
      print('Stack trace: $stackTrace');
      if (useFallback) {
        print('ğŸ”„ Exception caught, using fallback');
        return _createFallbackFace(imageFile);
      }
      return null;
    }
  }

  /// ğŸ†• Ù‚Øµ Ø§Ù„ÙˆØ¬Ù‡ Ù…Ù† Ø§Ù„ØµÙˆØ±Ø© (Ù…ØªÙˆØ§ÙÙ‚ Ù…Ø¹ face_management)
  static Future<img.Image?> cropFace(File imageFile, Rect faceRect) async {
    try {
      final imageBytes = await imageFile.readAsBytes();
      final originalImage = img.decodeImage(imageBytes);
      if (originalImage == null) {
        print('âŒ Failed to decode image for cropping');
        return null;
      }

      // Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª Ø¯Ø§Ø®Ù„ Ø­Ø¯ÙˆØ¯ Ø§Ù„ØµÙˆØ±Ø©
      final x = math.max(
        0,
        math.min(faceRect.left.toInt(), originalImage.width - 1),
      );
      final y = math.max(
        0,
        math.min(faceRect.top.toInt(), originalImage.height - 1),
      );
      final maxWidth = originalImage.width - x;
      final maxHeight = originalImage.height - y;
      final width = math.max(1, math.min(faceRect.width.toInt(), maxWidth));
      final height = math.max(1, math.min(faceRect.height.toInt(), maxHeight));

      print(
        'ğŸ“ Cropping: x=$x, y=$y, w=$width, h=$height (image: ${originalImage.width}x${originalImage.height})',
      );

      // Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯
      if (width <= 0 || height <= 0) {
        print('âŒ Invalid crop dimensions: ${width}x${height}');
        return null;
      }

      // Ù‚Øµ Ø§Ù„ÙˆØ¬Ù‡
      final croppedFace = img.copyCrop(
        originalImage,
        x: x,
        y: y,
        width: width,
        height: height,
      );

      print(
        'âœ… Face cropped successfully: ${croppedFace.width}x${croppedFace.height}',
      );
      return croppedFace;
    } catch (e, stackTrace) {
      print('âŒ Crop face error: $e');
      print('Stack trace: $stackTrace');
      return null;
    }
  }

  /// 2ï¸âƒ£ Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©: Landmark Detection
  static Future<List<Offset>?> detectLandmarks(img.Image faceImage) async {
    try {
      print('ğŸ“ Stage 2: Landmark Detection');

      if (_landmarkModel == null) {
        print('âŒ Landmark model not initialized');
        return null;
      }

      // ØªØºÙŠÙŠØ± Ø§Ù„Ø­Ø¬Ù… Ù„Ù€ 192x192
      final resized = img.copyResize(
        faceImage,
        width: LANDMARK_INPUT_SIZE,
        height: LANDMARK_INPUT_SIZE,
        interpolation: img.Interpolation.cubic,
      );

      final input = _imageToFloat32List(resized, LANDMARK_INPUT_SIZE);
      final inputTensor = input.reshape([
        1,
        LANDMARK_INPUT_SIZE,
        LANDMARK_INPUT_SIZE,
        3,
      ]);

      // ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
      final outputShape = _landmarkModel?.getOutputTensor(0).shape;
      if (outputShape == null) {
        print('âŒ Cannot get landmark output tensor shape');
        return null;
      }

      List<double> output;
      if (outputShape.length == 2) {
        final outputTensor = List.generate(
          1,
          (i) => List.filled(outputShape[1], 0.0),
        );
        try {
          _landmarkModel?.run(inputTensor, outputTensor);
        } catch (e) {
          print('âŒ Landmark model run error: $e');
          return null;
        }
        output = outputTensor[0];
      } else {
        final outputTensor = List.filled(
          outputShape.reduce((a, b) => a * b),
          0.0,
        );
        try {
          _landmarkModel?.run(inputTensor, outputTensor);
        } catch (e) {
          print('âŒ Landmark model run error: $e');
          return null;
        }
        output = outputTensor;
      }

      // ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ landmarks (106 Ù†Ù‚Ø·Ø© Ã— 2 Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª)
      List<Offset> landmarks = [];
      for (int i = 0; i < output.length; i += 2) {
        landmarks.add(Offset(output[i], output[i + 1]));
      }

      print('âœ… Detected ${landmarks.length} landmarks');
      return landmarks;
    } catch (e) {
      print('âŒ Landmark detection error: $e');
      return null;
    }
  }

  /// 3ï¸âƒ£ Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø«Ø§Ù„Ø«Ø©: Face Recognition (Embedding)
  static Future<List<double>?> generateEmbedding(img.Image alignedFace) async {
    try {
      print('ğŸ¯ Stage 3: Face Recognition');

      if (_recognitionModel == null) {
        print('âŒ Recognition model not initialized');
        return null;
      }

      // ØªØºÙŠÙŠØ± Ø§Ù„Ø­Ø¬Ù… Ù„Ù€ 112x112
      final resized = img.copyResize(
        alignedFace,
        width: RECOGNITION_INPUT_SIZE,
        height: RECOGNITION_INPUT_SIZE,
        interpolation: img.Interpolation.cubic,
      );

      final input = _imageToFloat32List(resized, RECOGNITION_INPUT_SIZE);
      final inputTensor = input.reshape([
        1,
        RECOGNITION_INPUT_SIZE,
        RECOGNITION_INPUT_SIZE,
        3,
      ]);

      final outputShape = _recognitionModel?.getOutputTensor(0).shape;
      if (outputShape == null) {
        print('âŒ Cannot get recognition output tensor shape');
        return null;
      }
      print('ğŸ“Š Recognition output: $outputShape');

      List<double> rawEmbedding;
      if (outputShape.length == 4) {
        final output = List.generate(
          outputShape[0],
          (i) => List.generate(
            outputShape[1],
            (j) => List.generate(
              outputShape[2],
              (k) => List.filled(outputShape[3], 0.0),
            ),
          ),
        );
        try {
          _recognitionModel?.run(inputTensor, output);
        } catch (e) {
          print('âŒ Recognition model run error: $e');
          return null;
        }
        rawEmbedding = List<double>.from(output[0][0][0]);
      } else if (outputShape.length == 2) {
        final output = List.generate(
          1,
          (i) => List.filled(outputShape[1], 0.0),
        );
        try {
          _recognitionModel?.run(inputTensor, output);
        } catch (e) {
          print('âŒ Recognition model run error: $e');
          return null;
        }
        rawEmbedding = List<double>.from(output[0]);
      } else {
        print('âŒ Unsupported output shape');
        return null;
      }

      // L2 Normalization
      final normalized = _normalizeEmbedding(rawEmbedding);
      print('âœ… Embedding generated: ${normalized.length}D');
      return normalized;
    } catch (e) {
      print('âŒ Recognition error: $e');
      return null;
    }
  }

  /// ğŸ”„ Pipeline ÙƒØ§Ù…Ù„Ø©: Detection â†’ Landmarks â†’ Recognition
  static Future<List<double>?> processImageFull(
    File imageFile, {
    bool skipDetection = false,
  }) async {
    if (!_isInitialized) {
      await initialize();
    }

    try {
      print('=== ğŸš€ InsightFace Full Pipeline ===');

      List<Rect> faces;

      if (skipDetection) {
        print('âš¡ SKIP DETECTION MODE: Using full image');
        faces = await _createFallbackFace(imageFile);
      } else {
        // 1ï¸âƒ£ Face Detection
        final detectedFaces = await detectFaces(imageFile, useFallback: true);
        if (detectedFaces == null || detectedFaces.isEmpty) {
          print('âŒ No faces detected and fallback failed');
          return null;
        }
        faces = detectedFaces;
      }

      // Ø§Ø³ØªØ®Ø¯Ù… Ø£ÙˆÙ„ ÙˆØ¬Ù‡ ÙÙ‚Ø·
      final faceRect = faces[0];
      print(
        'ğŸ“¦ Using face: ${faceRect.width.toInt()}x${faceRect.height.toInt()} at (${faceRect.left.toInt()}, ${faceRect.top.toInt()})',
      );

      // Ù‚Øµ Ø§Ù„ÙˆØ¬Ù‡
      final croppedFace = await cropFace(imageFile, faceRect);
      if (croppedFace == null) {
        print('âŒ Failed to crop face');
        return null;
      }

      print('âœ… Face cropped: ${croppedFace.width}x${croppedFace.height}');

      // 2ï¸âƒ£ Landmark Detection (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
      final landmarks = await detectLandmarks(croppedFace);
      if (landmarks == null) {
        print('âš ï¸ Landmarks not detected, proceeding without alignment');
      }

      // 3ï¸âƒ£ Face Alignment (Ø§Ø®ØªÙŠØ§Ø±ÙŠ - Ø¥Ø°Ø§ ØªØ¨ÙŠ Ø¯Ù‚Ø© Ø£Ø¹Ù„Ù‰)
      final alignedFace = landmarks != null
          ? _alignFace(croppedFace, landmarks)
          : croppedFace;

      // 4ï¸âƒ£ Face Recognition
      final embedding = await generateEmbedding(alignedFace);
      if (embedding != null) {
        print('âœ… Full pipeline completed successfully!');
      }

      return embedding;
    } catch (e, stackTrace) {
      print('âŒ Pipeline error: $e');
      print('Stack trace: $stackTrace');
      return null;
    }
  }

  /// Ù…Ø­Ø§Ø°Ø§Ø© Ø§Ù„ÙˆØ¬Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Landmarks
  static img.Image _alignFace(img.Image face, List<Offset> landmarks) {
    // Ù‡Ù†Ø§ ÙŠÙ…ÙƒÙ† ØªØ·Ø¨ÙŠÙ‚ Affine Transformation
    // Ù„ÙƒÙ† Ù„Ù„Ø¨Ø³Ø§Ø·Ø©ØŒ Ù†Ø±Ø¬Ø¹ Ø§Ù„ÙˆØ¬Ù‡ ÙƒÙ…Ø§ Ù‡Ùˆ
    // ÙŠÙ…ÙƒÙ† ØªØ­Ø³ÙŠÙ† Ù‡Ø°Ø§ Ù„Ø§Ø­Ù‚Ø§Ù‹
    return face;
  }

  /// Ø¥Ù†Ø´Ø§Ø¡ fallback face Ù…Ù† ÙƒØ§Ù…Ù„ Ø§Ù„ØµÙˆØ±Ø©
  static Future<List<Rect>> _createFallbackFace(File imageFile) async {
    try {
      final imageBytes = await imageFile.readAsBytes();
      final originalImage = img.decodeImage(imageBytes);

      if (originalImage == null) {
        return [];
      }

      final width = originalImage.width.toDouble();
      final height = originalImage.height.toDouble();

      // Ø§Ø³ØªØ®Ø¯Ø§Ù… 90% Ù…Ù† Ø§Ù„ØµÙˆØ±Ø©
      final padding = 0.05;
      final paddedWidth = width * (1.0 - padding * 2);
      final paddedHeight = height * (1.0 - padding * 2);
      final paddedLeft = width * padding;
      final paddedTop = height * padding;

      return [Rect.fromLTWH(paddedLeft, paddedTop, paddedWidth, paddedHeight)];
    } catch (e) {
      print('âŒ Fallback face creation error: $e');
      return [];
    }
  }

  /// ØªØ­ÙˆÙŠÙ„ ØµÙˆØ±Ø© Ø¥Ù„Ù‰ Float32List
  static Float32List _imageToFloat32List(img.Image image, int size) {
    final input = Float32List(size * size * 3);
    int pixelIndex = 0;

    for (int y = 0; y < size; y++) {
      for (int x = 0; x < size; x++) {
        final pixel = image.getPixel(x, y);
        input[pixelIndex] = (pixel.r / 127.5) - 1.0;
        input[pixelIndex + 1] = (pixel.g / 127.5) - 1.0;
        input[pixelIndex + 2] = (pixel.b / 127.5) - 1.0;
        pixelIndex += 3;
      }
    }

    return input;
  }

  /// Ø§Ø³ØªØ®Ø±Ø§Ø¬ bounding boxes Ù…Ù† Ù†ØªØ§Ø¦Ø¬ Detection
  static List<Rect> _parseFaceDetections(
    dynamic output,
    int imgWidth,
    int imgHeight,
    List<int> outputShape,
  ) {
    List<Rect> faces = [];
    try {
      print('ğŸ” Parsing detections...');
      print('ğŸ“Š Output shape: $outputShape');

      // Ø·Ø¨Ø§Ø¹Ø© Ù†ÙˆØ¹ Ø§Ù„Ù€ output
      print('ğŸ“‹ Output type: ${output.runtimeType}');

      List<List<double>> detections = [];
      if (outputShape.length == 3) {
        // [1, num_boxes, values]
        final batchOutput = output as List;
        if (batchOutput.isEmpty) {
          print('âš ï¸ Empty output from detection model');
          return faces;
        }
        detections = List<List<double>>.from(
          batchOutput[0].map((det) => List<double>.from(det)),
        );
      } else if (outputShape.length == 2) {
        // [num_boxes, values]
        detections = List<List<double>>.from(
          output.map((det) => List<double>.from(det)),
        );
      } else {
        print('âŒ Unsupported output shape: $outputShape');
        return faces;
      }

      print('ğŸ“¦ Total detections: ${detections.length}');
      if (detections.isEmpty) {
        print('âš ï¸ No detections in output');
        return faces;
      }

      // Ø·Ø¨Ø§Ø¹Ø© Ø´ÙƒÙ„ Ø£ÙˆÙ„ detection Ù„Ù„ØªØ­Ù„ÙŠÙ„
      if (detections.isNotEmpty && detections[0].isNotEmpty) {
        print('ğŸ“‹ First detection length: ${detections[0].length}');
        print('ğŸ“‹ First detection values: ${detections[0].toString()}');
      }

      const double confidenceThreshold = 0.4;
      int validDetections = 0;

      for (int i = 0; i < detections.length; i++) {
        final detection = detections[i];

        // ØªØ­Ù‚Ù‚ Ù…Ù† Ø·ÙˆÙ„ Ø§Ù„Ù€ detection
        if (detection.length < 3) {
          print('âš ï¸ Detection $i has invalid length: ${detection.length}');
          continue;
        }

        // ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ù‚ÙŠÙ… Ù„ÙŠØ³Øª null Ø£Ùˆ NaN
        bool hasInvalidValues = detection.any(
          (val) => val.isNaN || val.isInfinite,
        );

        if (hasInvalidValues) {
          print('âš ï¸ Detection $i contains invalid values (null/NaN/Infinite)');
          continue;
        }

        // Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ø¯ÙŠØ¯ Ø´ÙƒÙ„ Ø§Ù„Ù€ output
        // Ø§Ù„Ø£Ø´ÙƒØ§Ù„ Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©:
        // 1. [score, x1, y1, x2, y2, ...]
        // 2. [x1, y1, x2, y2, score, ...]
        // 3. [x1, y1, w, h, score, ...]

        double score;
        double x1, y1, x2, y2;

        // Ø¬Ø±Ø¨ ÙƒÙ„ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª
        if (detection.length >= 5) {
          // Ø§Ø­ØªÙ…Ø§Ù„ 1: score ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©
          if (detection[0] >= 0 && detection[0] <= 1) {
            score = detection[0];
            x1 = detection[1];
            y1 = detection[2];
            x2 = detection[3];
            y2 = detection[4];
          }
          // Ø§Ø­ØªÙ…Ø§Ù„ 2: score ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ© (index 4)
          else if (detection[4] >= 0 && detection[4] <= 1) {
            x1 = detection[0];
            y1 = detection[1];
            x2 = detection[2];
            y2 = detection[3];
            score = detection[4];
          }
          // Ø§Ø­ØªÙ…Ø§Ù„ 3: ØªØ¬Ø±Ø¨Ø© x,y,w,h,score
          else {
            x1 = detection[0];
            y1 = detection[1];
            x2 = x1 + detection[2]; // width
            y2 = y1 + detection[3]; // height
            score = detection.length > 4 ? detection[4] : 0.9;
          }
        } else if (detection.length == 3) {
          // Ø´ÙƒÙ„ Ù…Ø®ØªØµØ± - Ø§ÙØªØ±Ø¶ score Ø¹Ø§Ù„ÙŠ
          score = 0.9;
          x1 = detection[0];
          y1 = detection[1];
          x2 = detection[0] + detection[2];
          y2 = detection[1] + detection[2];
        } else {
          continue;
        }

        if (score < confidenceThreshold) {
          continue;
        }

        // ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ pixel coordinates Ø¥Ø°Ø§ ÙƒØ§Ù†Øª normalized
        if (x1 <= 1.0 && y1 <= 1.0 && x2 <= 1.0 && y2 <= 1.0) {
          x1 *= imgWidth;
          y1 *= imgHeight;
          x2 *= imgWidth;
          y2 *= imgHeight;
        }

        // Ø­Ø³Ø§Ø¨ width Ùˆ height
        final width = (x2 - x1).abs();
        final height = (y2 - y1).abs();

        // ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯
        if (width <= 0 || height <= 0) {
          continue;
        }

        // Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª Ø¯Ø§Ø®Ù„ Ø­Ø¯ÙˆØ¯ Ø§Ù„ØµÙˆØ±Ø©
        final left = math.max(0.0, math.min(x1, x2));
        final top = math.max(0.0, math.min(y1, y2));
        final right = math.max(x1, x2);
        final bottom = math.max(y1, y2);

        // ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª Ø¶Ù…Ù† Ø­Ø¯ÙˆØ¯ Ø§Ù„ØµÙˆØ±Ø©
        final clippedLeft = math.max(0.0, math.min(left, imgWidth.toDouble()));
        final clippedTop = math.max(0.0, math.min(top, imgHeight.toDouble()));
        final clippedRight = math.max(
          0.0,
          math.min(right, imgWidth.toDouble()),
        );
        final clippedBottom = math.max(
          0.0,
          math.min(bottom, imgHeight.toDouble()),
        );

        final validWidth = clippedRight - clippedLeft;
        final validHeight = clippedBottom - clippedTop;

        // ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ù„Ø­Ø¬Ù…
        if (validWidth > 20 && validHeight > 20) {
          faces.add(
            Rect.fromLTRB(clippedLeft, clippedTop, clippedRight, clippedBottom),
          );
          validDetections++;
          print(
            'âœ… Face $validDetections: score=${score.toStringAsFixed(2)}, '
            'bbox=(${clippedLeft.toInt()}, ${clippedTop.toInt()}, ${validWidth.toInt()}x${validHeight.toInt()})',
          );
        }
      }

      print('âœ… Valid faces found: $validDetections');

      // ØªØ±ØªÙŠØ¨ Ø§Ù„ÙˆØ¬ÙˆÙ‡ Ø­Ø³Ø¨ Ø§Ù„Ø­Ø¬Ù… (Ø§Ù„Ø£ÙƒØ¨Ø± Ø£ÙˆÙ„Ø§Ù‹)
      if (faces.length > 1) {
        faces.sort((a, b) {
          final areaA = a.width * a.height;
          final areaB = b.width * b.height;
          return areaB.compareTo(areaA);
        });
      }
    } catch (e, stackTrace) {
      print('âŒ Error parsing detections: $e');
      print('ğŸ“ Stack trace: $stackTrace');
    }

    return faces;
  }

  /// L2 Normalization
  static List<double> _normalizeEmbedding(List<double> embedding) {
    double norm = 0.0;
    for (double value in embedding) {
      norm += value * value;
    }
    norm = math.sqrt(norm);

    if (norm == 0.0 || norm.isNaN || norm.isInfinite) {
      return embedding;
    }

    return embedding.map((value) => value / norm).toList();
  }

  /// Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
  static double calculateSimilarity(List<double> emb1, List<double> emb2) {
    if (emb1.length != emb2.length) return 0.0;

    double dotProduct = 0.0;
    for (int i = 0; i < emb1.length; i++) {
      dotProduct += emb1[i] * emb2[i];
    }

    return math.max(0.0, math.min(1.0, dotProduct));
  }

  /// ğŸ†• ØªØ®Ø²ÙŠÙ† embedding (Ù…ØªÙˆØ§ÙÙ‚ Ù…Ø¹ face_management)
  static Future<bool> storeFaceEmbedding(
    String personId,
    File imageFile, {
    bool skipDetection = false,
  }) async {
    final embedding = await processImageFull(
      imageFile,
      skipDetection: skipDetection,
    );
    if (embedding != null && embedding.isNotEmpty) {
      if (_storedMultipleEmbeddings.containsKey(personId)) {
        _storedMultipleEmbeddings[personId]!.add(embedding);
      } else {
        _storedMultipleEmbeddings[personId] = [embedding];
      }
      print(
        'âœ… Stored embedding for $personId (${_storedMultipleEmbeddings[personId]!.length} total)',
      );
      return true;
    }
    print('âŒ Failed to store embedding for $personId');
    return false;
  }

  /// Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ ÙˆØ¬Ù‡
  static Future<RecognitionResult?> recognizeFace(
    File imageFile, {
    double threshold = DEFAULT_THRESHOLD,
    bool skipDetection = false,
  }) async {
    final queryEmbedding = await processImageFull(
      imageFile,
      skipDetection: skipDetection,
    );
    if (queryEmbedding == null) {
      return null;
    }

    if (_storedMultipleEmbeddings.isEmpty) {
      return RecognitionResult(
        personId: 'unknown',
        similarity: 0.0,
        isMatch: false,
      );
    }

    String? bestMatchId;
    double highestSimilarity = -1.0;

    for (var entry in _storedMultipleEmbeddings.entries) {
      for (var embedding in entry.value) {
        final similarity = calculateSimilarity(queryEmbedding, embedding);
        if (similarity > highestSimilarity) {
          highestSimilarity = similarity;
          bestMatchId = entry.key;
        }
      }
    }

    final isMatch = highestSimilarity >= threshold;

    return RecognitionResult(
      personId: bestMatchId ?? 'unknown',
      similarity: highestSimilarity,
      isMatch: isMatch,
      threshold: threshold,
    );
  }

  /// ØªØ­Ù…ÙŠÙ„ embeddings
  static void loadMultipleEmbeddings(
    Map<String, List<List<double>>> embeddings,
  ) {
    _storedMultipleEmbeddings = Map.from(embeddings);
    print('âœ… Loaded ${embeddings.length} persons');
  }

  /// Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ embeddings
  static Map<String, dynamic> getStoredEmbeddings() {
    Map<String, dynamic> result = {};
    _storedMultipleEmbeddings.forEach((personId, embeddings) {
      result[personId] = embeddings;
    });
    return result;
  }

  /// Ø­Ø°Ù embeddings
  static void removeFaceEmbedding(String personId) {
    _storedMultipleEmbeddings.remove(personId);
    print('ğŸ—‘ï¸ Removed embeddings for $personId');
  }

  /// Ù…Ø³Ø­ ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
  static void clearStoredEmbeddings() {
    _storedMultipleEmbeddings.clear();
  }

  /// ØªÙ†Ø¸ÙŠÙ
  static void dispose() {
    _detectionModel?.close();
    _landmarkModel?.close();
    _recognitionModel?.close();
    _detectionModel = null;
    _landmarkModel = null;
    _recognitionModel = null;
    _isInitialized = false;
    _storedMultipleEmbeddings.clear();
  }

  static Map<String, dynamic> getStatistics() {
    int totalEmbeddings = 0;
    _storedMultipleEmbeddings.forEach((_, embeddings) {
      totalEmbeddings += embeddings.length;
    });

    return {
      'total_persons': _storedMultipleEmbeddings.length,
      'total_embeddings': totalEmbeddings,
      'embedding_size': EMBEDDING_SIZE,
    };
  }
}

class RecognitionResult {
  final String personId;
  final double similarity;
  final bool isMatch;
  final double threshold;

  RecognitionResult({
    required this.personId,
    required this.similarity,
    required this.isMatch,
    this.threshold = 0.35,
  });

  @override
  String toString() {
    return 'RecognitionResult(personId: $personId, similarity: ${(similarity * 100).toStringAsFixed(1)}%, isMatch: $isMatch)';
  }
}
